{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Creation & Cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT_h4SxIgWYW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "d24d50d9-c29c-4dee-bc7b-d382b68eb685"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s1MStvYKWGT",
        "colab_type": "text"
      },
      "source": [
        "#Modules Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpxdlo5B5cKZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a38274c-f0a0-4a92-ac1c-29a7642a9981"
      },
      "source": [
        "!pip install -q twarc\n",
        "!pip install -q jsonlines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for twarc (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arSOg0PHKVFN",
        "colab_type": "text"
      },
      "source": [
        "#Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkQi21naXgaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os,zipfile,glob\n",
        "import pandas as pd\n",
        "#import twarc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DBTIwHZwp09",
        "colab_type": "text"
      },
      "source": [
        "Unzip Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yaTXuE10LXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dir_name = '/content/drive/My Drive/IBM_Hackathon_2020/Zip_Files'\n",
        "extension = \".zip\"\n",
        "\n",
        "os.chdir(dir_name) # change directory from working dir to dir with files\n",
        "\n",
        "for item in os.listdir(dir_name): # loop through items in dir\n",
        "    if item.endswith(extension): # check for \".zip\" extension\n",
        "        file_name = os.path.abspath(item) # get full path of files\n",
        "        zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
        "        zip_ref.extractall('/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Csv_File') # extract file to dir\n",
        "        zip_ref.close() # close file\n",
        "        os.remove(file_name) # delete zipped file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwKYV-kmKUUH",
        "colab_type": "text"
      },
      "source": [
        "#Total IEEE Dataset \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyfwx1ungbDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Csv_File/*.csv\"\n",
        "csv_list = glob.glob(path) # collecting all files  same path \n",
        "print(len(csv_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6ejH8LTd_jS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.DataFrame()\n",
        "for f in csv_list:\n",
        "  data1 = pd.read_csv(f,header=None)#reading the csv file\n",
        "  data = pd.concat([data,data1],ignore_index=True)#concating the two data frames\n",
        "  data.reset_index(drop=True,inplace=True) #resetting the indexes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6DA6f7WblnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.rename({0:'tweetID',1:'sentiment_score'},axis=1,inplace=True)#renaming the indexes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0yVWgbrcNro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.to_csv('/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Dataset/data.csv',index=False)#converting the dataframe to CSV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr3mFYloKrae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for f in csv_list:\n",
        "  df = pd.read_csv(f,header=None) #reading the CSV file\n",
        "  df = df[0] #Only taking the Tweet Id's from the dataset\n",
        "  base = os.path.basename(f) #returning the name of the file\n",
        "  path = '/content/drive/My Drive/IBM_Hackathon_2020/Tweets_ID/'+base\n",
        "  df.to_csv(path,index=False) # converting the dataframe to CSV "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb0V0u_LfOiT",
        "colab_type": "text"
      },
      "source": [
        "# Hydrating Tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WPefntNfZsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Insert API Keys here { run : \"auto\"}\n",
        "\n",
        "# These keys are received after applying for a twitter developer account\n",
        "\n",
        "consumer_key = \"rm2bLDjA2BzljoA0GomL5o6W7\"\n",
        "consumer_secret = \"xiFBG4VKWPuQts1v3uqAesllpDp36y44YkFnzBtezSbSYW9dBV\"\n",
        "access_token = \"935519854064418816-sOBxmFMaDygAx3FQXRBjH0drpZ2OXpB\"\n",
        "access_token_secret = \"GbOTefzapdet9vpmR3H9OBRuJNJNs1cI4Adh5HrkIYPJz\"\n",
        "\n",
        "t = twarc.Twarc(consumer_key, consumer_secret, access_token, access_token_secret) #Initializing Twarc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DFwYd7m58WR3",
        "colab": {}
      },
      "source": [
        "import jsonlines, json\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/IBM_Hackathon_2020/Geo_Location_Data_set/april28-june18.csv',header=None) #Loading IEEE Geodata\n",
        "data = data[0] #Taking only Tweet IDs for Hydration\n",
        "ids = data.values.tolist() #Getting list of tweet ids from pandas DataFrame\n",
        "hydrated_tweets = [] #Creating empty list\n",
        "ids_to_hydrate = set(ids) #Creating ids_to_hydrate list\n",
        "\n",
        "# Now, use twarc and start hydrating\n",
        "for tweet in t.hydrate(ids_to_hydrate): #Initializing Hydrate Iterator with twarc\n",
        "  if tweet['place'] != None: #Checking for place value\n",
        "    if tweet['place']['country'] == 'India': #Checking if Country is India\n",
        "      hydrated_tweets.append(tweet) #Appending Obtained tweet to hydrated_tweets list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl0GE81YU7n1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_filename = \"/content/drive/My Drive/IBM_Hackathon_2020/Geo_Location_Data_set/output.csv\" #output file path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9dah8XJlH4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert jsonl to csv\n",
        "import csv, jsonlines\n",
        "\n",
        "# These are the column name that are selected to be stored in the csv\n",
        "keyset = [\"created_at\", \"id\", \"id_str\", \"full_text\", \"source\", \"truncated\", \"in_reply_to_status_id\",\n",
        "          \"in_reply_to_status_id_str\", \"in_reply_to_user_id\", \"in_reply_to_user_id_str\", \n",
        "          \"in_reply_to_screen_name\", \"user\", \"coordinates\", \"place\", \"quoted_status_id\",\n",
        "          \"quoted_status_id_str\", \"is_quote_status\", \"quoted_status\", \"retweeted_status\", \n",
        "          \"quote_count\", \"reply_count\", \"retweet_count\", \"favorite_count\", \"entities\", \n",
        "          \"extended_entities\", \"favorited\", \"retweeted\", \"possibly_sensitive\", \"filter_level\", \n",
        "          \"lang\", \"matching_rules\", \"current_user_retweet\", \"scopes\", \"withheld_copyright\", \n",
        "          \"withheld_in_countries\", \"withheld_scope\", \"geo\", \"contributors\", \"display_text_range\",\n",
        "          \"quoted_status_permalink\"]\n",
        "\n",
        "# Writes them out (Saving output CSV file with the Indian COVID-19 Tweets)\n",
        "with  open(output_filename, \"w+\") as output_file:\n",
        "    d = csv.DictWriter(output_file, keyset)\n",
        "    d.writeheader()\n",
        "    d.writerows(hydrated_tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1znR6POE3D6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(output_filename)# reading the file\n",
        "data1 = pd.read_csv('/content/drive/My Drive/IBM_Hackathon_2020/Geo_Location_Data_set/april28-june18.csv',header=None)\n",
        "data1.rename({0:'id',1:'Sentiment'},inplace=True,axis=1)#renaming the columns\n",
        "result = pd.merge(data,data1,on='id')#merging the two dataframes\n",
        "result.drop_duplicates(subset =\"id\", keep = False, inplace = True)#removing duplicates\n",
        "result.to_csv('/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Dataset/India_Dataset.csv',index=False)# Converting to CSV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h2mL5R4Pma5",
        "colab_type": "text"
      },
      "source": [
        "#*Sentiment Addition*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6BD7X7zez4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/hydrated_corona_tweets_01\")\n",
        "df = df[['id','retweet_count','lang','text']]#selection of Features\n",
        "data = df[df[\"lang\"]==\"en\"]#Extracting the tweets from dataframe whose language is English"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tjyzx_qkdjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweet_ids_file = \"/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Csv_File/corona_tweets_01.csv\" #Reference Original CSV Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwtOSUSHLFjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(input_filename)# reading the file\n",
        "data.rename({'text':'full_text'},inplace=True,axis=1)\n",
        "dataset = data[['id','full_text','retweet_count']]\n",
        "df = pd.read_csv(tweet_ids_file,header=None)\n",
        "df.rename({0:'id',1:'Sentiment'},inplace=True,axis=1)#renaming the columns\n",
        "output = pd.merge(dataset,df,on='id')#merging (inner) the two dataframes\n",
        "output.drop_duplicates(subset =\"id\", keep = False, inplace = True)#removing duplicates\n",
        "output.to_csv('/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Dataset/Dataset.csv',index=False)# Converting to CSV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il0DDPOA64Cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to Convert the IEEE Sentiment score to Sentiment Text namely positive, negative, neutral\n",
        "def func(x):\n",
        "    if x < 0:\n",
        "        return \"negative\"\n",
        "    elif x == 0:\n",
        "        return \"neutral\"\n",
        "    else:\n",
        "        return \"positive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RzONZynCZp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = pd.read_csv(\"/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Dataset/India_Dataset.csv\")# reading the file\n",
        "y= df1.Sentiment\n",
        "X= df1.drop('Sentiment',axis=1)\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.75,random_state=111)#Spliting the dataset \n",
        "df2 = pd.concat([X_train,y_train],axis=1)#concating the two datasets\n",
        "df3 = pd.concat([X_test,y_test],axis=1)#concating the two datasets\n",
        "df3.reset_index(drop=True,inplace=True)#resetting the indexes\n",
        "df3['Sentiment'] = df3['Sentiment'].apply(lambda x : func(x))#converting sentiment score to sentiment text\n",
        "df3.to_csv('/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Test_India.csv',index=False)# Converting to CSV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVqGmdmTY3s5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Dataset/Dataset.csv')# reading the file\n",
        "df4=data.sample(n=58944,random_state=123)# sampling the dataset\n",
        "df5=pd.concat([df2,df4],ignore_index=True)#Concating the two datasets \n",
        "df5.reset_index(drop=True,inplace=True)# restting the indexes\n",
        "df5['Sentiment'] = df5['Sentiment'].apply(lambda x : func(x))#converting sentiment score to sentiment text\n",
        "df5.to_csv(\"/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Data.csv\",index=False)# Converting to CSV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf7r51jg9pSH",
        "colab_type": "text"
      },
      "source": [
        "# Splitting Final Data to Train and Test sets for DL Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhwlvG_O9vdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Data.csv')# reading the file\n",
        "y = data.Sentiment\n",
        "X = data.drop('Sentiment',axis=1)\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=123)\n",
        "df1 = pd.concat([X_train,y_train],axis=1)#concating the two datasets\n",
        "df1.reset_index(drop=True,inplace=True)#resetting the indexes\n",
        "df1.to_csv('/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Train_Data.csv',index=False) # Saving Training/Validation data for DL Model\n",
        "df2 = pd.concat([X_test,y_test],axis=1)#concating the two datasets\n",
        "df2.reset_index(drop=True,inplace=True)#resetting the indexes\n",
        "df2.to_csv('/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Test_Data.csv',index=False)# Saving Test data for DL Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBmOnd5K_xaZ",
        "colab_type": "text"
      },
      "source": [
        "#*Data Cleaning*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvGUzpgJ_3QS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining functions for Cleaning and Normalization of Data\n",
        "import re\n",
        "import string\n",
        "def replace_url(string): # cleaning of URL\n",
        "    text = re.sub(r'http\\S+', 'LINK', string)\n",
        "    return text\n",
        "\n",
        "\n",
        "def replace_email(text):#Cleaning of Email related text\n",
        "    line = re.sub(r'[\\w\\.-]+@[\\w\\.-]+','MAIL',str(text))\n",
        "    return \"\".join(line)\n",
        "\n",
        "def rep(text):#cleaning of non standard words\n",
        "    grp = text.group(0)\n",
        "    if len(grp) > 3:\n",
        "        return grp[0:2]\n",
        "    else:\n",
        "        return grp# can change the value here on repetition\n",
        "def unique_char(rep,sentence):\n",
        "    convert = re.sub(r'(\\w)\\1+', rep, sentence) \n",
        "    return convert\n",
        "\n",
        "def find_dollar(text):#Finding the dollar sign in the text\n",
        "    line=re.sub(r'\\$\\d+(?:\\.\\d+)?','PRICE',text)\n",
        "    return \"\".join(line)\n",
        "\n",
        "def replace_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "    u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
        "    u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
        "    u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
        "    u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U000024C2-\\U0001F251\"\n",
        "    \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'EMOJI', text) \n",
        "\n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n",
        "          '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n",
        "          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n",
        "          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n",
        "          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n",
        "          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
        "          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n",
        "          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = str(text)\n",
        "    for punct in puncts + list(string.punctuation):\n",
        "        if punct in text:\n",
        "            text = text.replace(punct, f'')\n",
        "    return text\n",
        "   \n",
        "def replace_asterisk(text):\n",
        "    text = re.sub(\"\\*\", 'ABUSE ', text)\n",
        "    return text\n",
        "\n",
        "def remove_duplicates(text):\n",
        "    text = re.sub(r'\\b(\\w+\\s*)\\1{1,}', '\\\\1', text)\n",
        "    return text\n",
        "\n",
        "def change(text):\n",
        "    if(text == ''):\n",
        "        return text\n",
        "  #calling the subfunctions in the cleaning function\n",
        "    text = replace_email(text)\n",
        "    text = replace_url(text)\n",
        "    text = unique_char(rep,text)\n",
        "    text = replace_asterisk(text)\n",
        "    text = remove_duplicates(text)\n",
        "    text = clean_text(text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drCGOxm4Jfpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Loading different csv files for cleaning \n",
        "pathname = \"/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Data.csv\"\n",
        "# pathname = \"/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Train_India.csv\"\n",
        "# pathname = \"/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Test_Data.csv\"\n",
        "# pathname = \"/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Test_India.csv\"\n",
        "Data = pd.read_csv(pathname) #reading the file\n",
        "Data['full_text'] = Data['full_text'].apply(lambda x : change(x)) # Running cleaning and normalization function on datasets\n",
        "Data.to_csv(pathname) #converting to CSV "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}