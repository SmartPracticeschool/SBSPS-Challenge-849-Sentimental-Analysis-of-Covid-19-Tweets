{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KkQi21naXgaL"
   },
   "outputs": [],
   "source": [
    "import os,zipfile\n",
    "import pandas as pd\n",
    "#import twarc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DBTIwHZwp09"
   },
   "source": [
    "Unzip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yaTXuE10LXi"
   },
   "outputs": [],
   "source": [
    "\n",
    "dir_name = '/content/drive/My Drive/IBM_Hackathon_2020/Zip_Files'\n",
    "extension = \".zip\"\n",
    "\n",
    "os.chdir(dir_name) # change directory from working dir to dir with files\n",
    "\n",
    "for item in os.listdir(dir_name): # loop through items in dir\n",
    "    if item.endswith(extension): # check for \".zip\" extension\n",
    "        file_name = os.path.abspath(item) # get full path of files\n",
    "        zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "        zip_ref.extractall('/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Csv_File') # extract file to dir\n",
    "        zip_ref.close() # close file\n",
    "        os.remove(file_name) # delete zipped file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SwKYV-kmKUUH"
   },
   "source": [
    "#Total IEEE Dataset \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gyfwx1ungbDB"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Csv_File/*.csv\"\n",
    "csv_list = glob.glob(path) # collecting all files  same path \n",
    "print(len(csv_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6ejH8LTd_jS"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "for f in csv_list:\n",
    "  data1 = pd.read_csv(f,header=None)#reading the csv file\n",
    "  data = pd.concat([data,data1],ignore_index=True)#concating the two data frames\n",
    "  data.reset_index(drop=True,inplace=True) #resetting the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6DA6f7WblnV"
   },
   "outputs": [],
   "source": [
    "data.rename({0:'tweetID',1:'sentiment_score'},axis=1,inplace=True)#renaming the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0yVWgbrcNro"
   },
   "outputs": [],
   "source": [
    "data.to_csv('/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Dataset/data.csv',index=False)#converting the dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tr3mFYloKrae"
   },
   "outputs": [],
   "source": [
    "for f in csv_list:\n",
    "  df = pd.read_csv(f,header=None) #reading the CSV file\n",
    "  df = df[0] #Only taking the Tweet Id's from the dataset\n",
    "  base = os.path.basename(f) #returning the name of the file\n",
    "  path = '/content/drive/My Drive/IBM_Hackathon_2020/Tweets_ID/'+base\n",
    "  df.to_csv(path,index=False) # converting the dataframe to CSV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qb0V0u_LfOiT"
   },
   "source": [
    "# Hydrating Tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WPefntNfZsZ"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "# Insert API Keys here { run : \"auto\"}\n",
    "\n",
    "# These keys are received after applying for a twitter developer account\n",
    "\n",
    "consumer_key = \"rm2bLDjA2BzljoA0GomL5o6W7\"\n",
    "consumer_secret = \"xiFBG4VKWPuQts1v3uqAesllpDp36y44YkFnzBtezSbSYW9dBV\"\n",
    "access_token = \"935519854064418816-sOBxmFMaDygAx3FQXRBjH0drpZ2OXpB\"\n",
    "access_token_secret = \"GbOTefzapdet9vpmR3H9OBRuJNJNs1cI4Adh5HrkIYPJz\"\n",
    "\n",
    "t = twarc.Twarc(consumer_key, consumer_secret, access_token, access_token_secret) #Initializing Twarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DFwYd7m58WR3"
   },
   "outputs": [],
   "source": [
    "import jsonlines, json\n",
    "\n",
    "data = pd.read_csv('/content/drive/My Drive/IBM_Hackathon_2020/Geo_Location_Data_set/april28-june18.csv',header=None) #Loading IEEE Geodata\n",
    "data = data[0] #Taking only Tweet IDs for Hydration\n",
    "ids = data.values.tolist() #Getting list of tweet ids from pandas DataFrame\n",
    "hydrated_tweets = [] #Creating empty list\n",
    "ids_to_hydrate = set(ids) #Creating ids_to_hydrate list\n",
    "\n",
    "# Now, use twarc and start hydrating\n",
    "for tweet in t.hydrate(ids_to_hydrate): #Initializing Hydrate Iterator with twarc\n",
    "  if tweet['place'] != None: #Checking for place value\n",
    "    if tweet['place']['country'] == 'India': #Checking if Country is India\n",
    "      hydrated_tweets.append(tweet) #Appending Obtained tweet to hydrated_tweets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wl0GE81YU7n1"
   },
   "outputs": [],
   "source": [
    "output_filename = \"/content/drive/My Drive/IBM_Hackathon_2020/Geo_Location_Data_set/output.csv\" #output file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9dah8XJlH4n"
   },
   "outputs": [],
   "source": [
    "# Convert jsonl to csv\n",
    "import csv, jsonlines\n",
    "\n",
    "# These are the column name that are selected to be stored in the csv\n",
    "keyset = [\"created_at\", \"id\", \"id_str\", \"full_text\", \"source\", \"truncated\", \"in_reply_to_status_id\",\n",
    "          \"in_reply_to_status_id_str\", \"in_reply_to_user_id\", \"in_reply_to_user_id_str\", \n",
    "          \"in_reply_to_screen_name\", \"user\", \"coordinates\", \"place\", \"quoted_status_id\",\n",
    "          \"quoted_status_id_str\", \"is_quote_status\", \"quoted_status\", \"retweeted_status\", \n",
    "          \"quote_count\", \"reply_count\", \"retweet_count\", \"favorite_count\", \"entities\", \n",
    "          \"extended_entities\", \"favorited\", \"retweeted\", \"possibly_sensitive\", \"filter_level\", \n",
    "          \"lang\", \"matching_rules\", \"current_user_retweet\", \"scopes\", \"withheld_copyright\", \n",
    "          \"withheld_in_countries\", \"withheld_scope\", \"geo\", \"contributors\", \"display_text_range\",\n",
    "          \"quoted_status_permalink\"]\n",
    "\n",
    "# Writes them out (Saving output CSV file with the Indian COVID-19 Tweets)\n",
    "with  open(output_filename, \"w+\") as output_file:\n",
    "    d = csv.DictWriter(output_file, keyset)\n",
    "    d.writeheader()\n",
    "    d.writerows(hydrated_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1znR6POE3D6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(output_filename)# reading the file\n",
    "data1 = pd.read_csv('/content/drive/My Drive/IBM_Hackathon_2020/Geo_Location_Data_set/april28-june18.csv',header=None)\n",
    "data1.rename({0:'id',1:'Sentiment'},inplace=True,axis=1)#renaming the columns\n",
    "result = pd.merge(data,data1,on='id')#merging the two dataframes\n",
    "result.drop_duplicates(subset =\"id\", keep = False, inplace = True)#removing duplicates\n",
    "result.to_csv('/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Dataset/India_Dataset.csv',index=False)# Converting to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1h2mL5R4Pma5"
   },
   "source": [
    "#*Sentiment Addition*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o6BD7X7zez4i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/My Drive/hydrated_corona_tweets_01\")\n",
    "df = df[['id','retweet_count','lang','text']]#selection of Features\n",
    "data = df[df[\"lang\"]==\"en\"]#Extracting the tweets from dataframe whose language is English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4tjyzx_qkdjL"
   },
   "outputs": [],
   "source": [
    "tweet_ids_file = \"/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Csv_File/corona_tweets_01.csv\" #Reference Original CSV Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GwtOSUSHLFjn"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(input_filename)# reading the file\n",
    "data.rename({'text':'full_text'},inplace=True,axis=1)\n",
    "dataset = data[['id','full_text','retweet_count']]\n",
    "df = pd.read_csv(tweet_ids_file,header=None)\n",
    "df.rename({0:'id',1:'Sentiment'},inplace=True,axis=1)#renaming the columns\n",
    "output = pd.merge(dataset,df,on='id')#merging (inner) the two dataframes\n",
    "output.drop_duplicates(subset =\"id\", keep = False, inplace = True)#removing duplicates\n",
    "output.to_csv('/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Dataset/Dataset.csv',index=False)# Converting to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "il0DDPOA64Cf"
   },
   "outputs": [],
   "source": [
    "# Function to Convert the IEEE Sentiment score to Sentiment Text namely positive, negative, neutral\n",
    "def func(x):\n",
    "    if x < 0:\n",
    "        return \"negative\"\n",
    "    elif x == 0:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6RzONZynCZp1"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Dataset/India_Dataset.csv\")# reading the file\n",
    "y= df1.Sentiment\n",
    "X= df1.drop('Sentiment',axis=1)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.75,random_state=111)#Spliting the dataset \n",
    "df2 = pd.concat([X_train,y_train],axis=1)#concating the two datasets\n",
    "df3 = pd.concat([X_test,y_test],axis=1)#concating the two datasets\n",
    "df3.reset_index(drop=True,inplace=True)#resetting the indexes\n",
    "df3['Sentiment'] = df3['Sentiment'].apply(lambda x : func(x))#converting sentiment score to sentiment text\n",
    "df3.to_csv('/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Test_India.csv',index=False)# Converting to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVqGmdmTY3s5"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/drive/My Drive/IBM_Hackathon_2020/Tweet_Dataset/Dataset.csv')# reading the file\n",
    "df4=data.sample(n=58944,random_state=123)# sampling the dataset\n",
    "df5=pd.concat([df2,df4],ignore_index=True)#Concating the two datasets \n",
    "df5.reset_index(drop=True,inplace=True)# restting the indexes\n",
    "df5['Sentiment'] = df5['Sentiment'].apply(lambda x : func(x))#converting sentiment score to sentiment text\n",
    "df5.to_csv(\"/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Data.csv\",index=False)# Converting to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zf7r51jg9pSH"
   },
   "source": [
    "# Splitting Final Data to Train and Test sets for DL Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZhwlvG_O9vdD"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Data.csv')# reading the file\n",
    "y = data.Sentiment\n",
    "X = data.drop('Sentiment',axis=1)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=123)\n",
    "df1 = pd.concat([X_train,y_train],axis=1)#concating the two datasets\n",
    "df1.reset_index(drop=True,inplace=True)#resetting the indexes\n",
    "df1.to_csv('/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Train_Data.csv',index=False) # Saving Training/Validation data for DL Model\n",
    "df2 = pd.concat([X_test,y_test],axis=1)#concating the two datasets\n",
    "df2.reset_index(drop=True,inplace=True)#resetting the indexes\n",
    "df2.to_csv('/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Test_Data.csv',index=False)# Saving Test data for DL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GBmOnd5K_xaZ"
   },
   "source": [
    "#*Data Cleaning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvGUzpgJ_3QS"
   },
   "outputs": [],
   "source": [
    "#Defining functions for Cleaning and Normalization of Data\n",
    "import re\n",
    "import string\n",
    "def replace_url(string): # cleaning of URL\n",
    "    text = re.sub(r'http\\S+', 'LINK', string)\n",
    "    return text\n",
    "\n",
    "\n",
    "def replace_email(text):#Cleaning of Email related text\n",
    "    line = re.sub(r'[\\w\\.-]+@[\\w\\.-]+','MAIL',str(text))\n",
    "    return \"\".join(line)\n",
    "\n",
    "def rep(text):#cleaning of non standard words\n",
    "    grp = text.group(0)\n",
    "    if len(grp) > 3:\n",
    "        return grp[0:2]\n",
    "    else:\n",
    "        return grp# can change the value here on repetition\n",
    "def unique_char(rep,sentence):\n",
    "    convert = re.sub(r'(\\w)\\1+', rep, sentence) \n",
    "    return convert\n",
    "\n",
    "def find_dollar(text):#Finding the dollar sign in the text\n",
    "    line=re.sub(r'\\$\\d+(?:\\.\\d+)?','PRICE',text)\n",
    "    return \"\".join(line)\n",
    "\n",
    "def replace_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'EMOJI', text) \n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n",
    "          '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n",
    "          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n",
    "          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n",
    "          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n",
    "          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n",
    "          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text)\n",
    "    for punct in puncts + list(string.punctuation):\n",
    "        if punct in text:\n",
    "            text = text.replace(punct, f'')\n",
    "    return text\n",
    "   \n",
    "def replace_asterisk(text):\n",
    "    text = re.sub(\"\\*\", 'ABUSE ', text)\n",
    "    return text\n",
    "\n",
    "def remove_duplicates(text):\n",
    "    text = re.sub(r'\\b(\\w+\\s*)\\1{1,}', '\\\\1', text)\n",
    "    return text\n",
    "\n",
    "def change(text):\n",
    "    if(text == ''):\n",
    "        return text\n",
    "  #calling the subfunctions in the cleaning function\n",
    "    text = replace_email(text)\n",
    "    text = replace_url(text)\n",
    "    text = unique_char(rep,text)\n",
    "    text = replace_asterisk(text)\n",
    "    text = remove_duplicates(text)\n",
    "    text = clean_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drCGOxm4Jfpe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading different csv files for cleaning \n",
    "pathname = \"/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Data.csv\"\n",
    "# pathname = \"/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Train_India.csv\"\n",
    "# pathname = \"/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Test_Data.csv\"\n",
    "# pathname = \"/content/drive/My Drive/IBM_Hackathon_2020/Final_Datasets/Test_India.csv\"\n",
    "Data = pd.read_csv(pathname) #reading the file\n",
    "Data['full_text'] = Data['full_text'].apply(lambda x : change(x)) # Running cleaning and normalization function on datasets\n",
    "Data.to_csv(pathname) #converting to CSV "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Creation & Cleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
